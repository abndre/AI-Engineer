{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8da146",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67fc658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=\"<OPENAI_API_TOKEN>\")\n",
    "\n",
    "# Predict the words following the text in question\n",
    "prompt = 'Three reasons for using LangChain for LLM application development.'\n",
    "response = llm.invoke(prompt)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9618c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9512f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template from the template string\n",
    "template = \"You are an artificial intelligence assistant, answer the question. {question}\"\n",
    "prompt = PromptTemplate.from_template(\n",
    "    template=template\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key='<OPENAI_API_TOKEN>')\t\n",
    "\n",
    "# Create a chain to integrate the prompt template and LLM\n",
    "llm_chain = prompt | llm\n",
    "\n",
    "# Invoke the chain on the question\n",
    "question = \"How does LangChain make LLM application development easier?\"\n",
    "print(llm_chain.invoke({\"question\": question}))\n",
    "\n",
    "# <script.py> output:\n",
    "#     content='LangChain simplifies the development of applications using large language models (LLMs) by providing a structured framework and a set of tools that address various aspects of LLM application building. Here are some key ways in which LangChain makes this process easier:\\n\\n1. **Modular Components**: LangChain offers modular components that allow developers to build applications by reusing existing pieces. This includes modules for text generation, question answering, and more, helping to speed up development and reduce redundancy.\\n\\n2. **Chains**: LangChain supports the creation of chains, which are sequences of function calls that can be combined in various ways to perform complex tasks. This makes it easy to build multi-step workflows, enabling developers to dictate how LLMs interact with other components of the application.\\n\\n3. **Agents**: LangChain facilitates the creation of agents that can leverage LLMs for decision-making. These agents can dynamically choose actions based on user input, making the application more interactive and responsive.\\n\\n4. **Integration with Data Sources**: LangChain provides straightforward integrations with various data sources and APIs, enabling developers to fetch and process external information to enhance the capabilities of their LLM applications.\\n\\n5. **Prompt Management**: LangChain offers tools for managing and optimizing prompts, which are crucial for extracting the desired behavior from LLMs. This includes capabilities for prompt templates and prompt versioning.\\n\\n6. **Memory Management**: The framework includes features for managing context and memory, allowing applications to keep track of past interactions and maintain continuity in conversations or tasks.\\n\\n7. **Evaluation and Debugging Tools**: LangChain provides tools for evaluating and debugging LLM performance, helping developers identify issues and optimize their models more effectively.\\n\\n8. **Flexibility and Customization**: Developers can easily customize components and workflows to suit specific needs, making it versatile for a range of applications, from chatbots to more complex AI-driven solutions.\\n\\nOverall, by providing a robust framework and a collection of specialized tools, LangChain helps developers to focus on building and innovating, rather than getting bogged down in the low-level implementation details of LLM interactions.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 426, 'prompt_tokens': 29, 'total_tokens': 455, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'finish_reason': 'stop', 'logprobs': None} id='run-cbd5a6b7-e88c-4af9-aa30-5248e395ffac-0' usage_metadata={'input_tokens': 29, 'output_tokens': 426, 'total_tokens': 455, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33293d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key='<OPENAI_API_TOKEN>')\n",
    "\n",
    "# Create a chat prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a geography expert that returns the colors present in a country's flag.\"),\n",
    "        (\"human\", \"France\"),\n",
    "        (\"ai\", \"blue, white, red\"),\n",
    "        (\"human\", \"{country}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Chain the prompt template and model, and invoke the chain\n",
    "llm_chain = prompt_template | llm\n",
    "\n",
    "country = \"Japan\"\n",
    "response = llm_chain.invoke({\"country\": country})\n",
    "print(response.content)\n",
    "\n",
    "# output:\n",
    "#     red, white"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
